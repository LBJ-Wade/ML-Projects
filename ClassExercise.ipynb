{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked ensemble tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>ABAT</th>\n",
       "      <th>ABHD2</th>\n",
       "      <th>ACTB</th>\n",
       "      <th>ACTR2</th>\n",
       "      <th>ACTR5</th>\n",
       "      <th>ACVR2A</th>\n",
       "      <th>ADAMDEC1</th>\n",
       "      <th>ADCYAP1R1</th>\n",
       "      <th>AEBP1</th>\n",
       "      <th>...</th>\n",
       "      <th>WT1</th>\n",
       "      <th>XPO7</th>\n",
       "      <th>XPOT</th>\n",
       "      <th>YTHDC2</th>\n",
       "      <th>ZDHHC14</th>\n",
       "      <th>ZDHHC7</th>\n",
       "      <th>ZEB1</th>\n",
       "      <th>ZFP36</th>\n",
       "      <th>ZHX3</th>\n",
       "      <th>ZNF423</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PRO.C5</td>\n",
       "      <td>-0.010674</td>\n",
       "      <td>0.263376</td>\n",
       "      <td>-0.115492</td>\n",
       "      <td>-0.323565</td>\n",
       "      <td>0.005161</td>\n",
       "      <td>-0.504271</td>\n",
       "      <td>-1.283720</td>\n",
       "      <td>-0.433908</td>\n",
       "      <td>0.673072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077048</td>\n",
       "      <td>0.459961</td>\n",
       "      <td>-0.072049</td>\n",
       "      <td>0.243935</td>\n",
       "      <td>-0.056318</td>\n",
       "      <td>-0.204971</td>\n",
       "      <td>0.179639</td>\n",
       "      <td>-0.292136</td>\n",
       "      <td>-0.034261</td>\n",
       "      <td>0.490152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MES.C1</td>\n",
       "      <td>-0.710741</td>\n",
       "      <td>0.110421</td>\n",
       "      <td>0.532555</td>\n",
       "      <td>-0.253877</td>\n",
       "      <td>-0.389024</td>\n",
       "      <td>-0.121941</td>\n",
       "      <td>-1.732920</td>\n",
       "      <td>-0.727880</td>\n",
       "      <td>1.706110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.547120</td>\n",
       "      <td>-0.674773</td>\n",
       "      <td>-0.236746</td>\n",
       "      <td>0.551354</td>\n",
       "      <td>0.215982</td>\n",
       "      <td>0.196677</td>\n",
       "      <td>1.467320</td>\n",
       "      <td>2.461040</td>\n",
       "      <td>0.415041</td>\n",
       "      <td>2.116880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DIF.C4</td>\n",
       "      <td>0.881506</td>\n",
       "      <td>0.372862</td>\n",
       "      <td>0.052344</td>\n",
       "      <td>0.028721</td>\n",
       "      <td>-0.848119</td>\n",
       "      <td>-1.281180</td>\n",
       "      <td>1.524370</td>\n",
       "      <td>-0.288317</td>\n",
       "      <td>-2.010830</td>\n",
       "      <td>...</td>\n",
       "      <td>1.058170</td>\n",
       "      <td>0.350895</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>0.010498</td>\n",
       "      <td>0.592285</td>\n",
       "      <td>-0.338954</td>\n",
       "      <td>-0.842242</td>\n",
       "      <td>0.096242</td>\n",
       "      <td>-0.471005</td>\n",
       "      <td>-1.662190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MES.C1</td>\n",
       "      <td>-1.085090</td>\n",
       "      <td>0.415651</td>\n",
       "      <td>0.395376</td>\n",
       "      <td>-0.271050</td>\n",
       "      <td>0.146536</td>\n",
       "      <td>-0.363270</td>\n",
       "      <td>0.993823</td>\n",
       "      <td>-0.450427</td>\n",
       "      <td>1.999170</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.677226</td>\n",
       "      <td>-0.109778</td>\n",
       "      <td>0.033163</td>\n",
       "      <td>0.760080</td>\n",
       "      <td>-1.169030</td>\n",
       "      <td>0.325604</td>\n",
       "      <td>1.785760</td>\n",
       "      <td>-0.212328</td>\n",
       "      <td>0.537493</td>\n",
       "      <td>-0.102138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MES.C1</td>\n",
       "      <td>-0.932230</td>\n",
       "      <td>0.045352</td>\n",
       "      <td>0.595068</td>\n",
       "      <td>0.187856</td>\n",
       "      <td>-0.200287</td>\n",
       "      <td>0.211144</td>\n",
       "      <td>1.844640</td>\n",
       "      <td>-0.416482</td>\n",
       "      <td>1.327800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.961688</td>\n",
       "      <td>-0.009010</td>\n",
       "      <td>0.529045</td>\n",
       "      <td>-0.551470</td>\n",
       "      <td>-0.188697</td>\n",
       "      <td>0.157393</td>\n",
       "      <td>0.469166</td>\n",
       "      <td>1.748000</td>\n",
       "      <td>0.144196</td>\n",
       "      <td>-0.561641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 322 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    class      ABAT     ABHD2      ACTB     ACTR2     ACTR5    ACVR2A  \\\n",
       "0  PRO.C5 -0.010674  0.263376 -0.115492 -0.323565  0.005161 -0.504271   \n",
       "1  MES.C1 -0.710741  0.110421  0.532555 -0.253877 -0.389024 -0.121941   \n",
       "2  DIF.C4  0.881506  0.372862  0.052344  0.028721 -0.848119 -1.281180   \n",
       "3  MES.C1 -1.085090  0.415651  0.395376 -0.271050  0.146536 -0.363270   \n",
       "4  MES.C1 -0.932230  0.045352  0.595068  0.187856 -0.200287  0.211144   \n",
       "\n",
       "   ADAMDEC1  ADCYAP1R1     AEBP1    ...          WT1      XPO7      XPOT  \\\n",
       "0 -1.283720  -0.433908  0.673072    ...     0.077048  0.459961 -0.072049   \n",
       "1 -1.732920  -0.727880  1.706110    ...     0.547120 -0.674773 -0.236746   \n",
       "2  1.524370  -0.288317 -2.010830    ...     1.058170  0.350895 -0.000051   \n",
       "3  0.993823  -0.450427  1.999170    ...    -0.677226 -0.109778  0.033163   \n",
       "4  1.844640  -0.416482  1.327800    ...     0.961688 -0.009010  0.529045   \n",
       "\n",
       "     YTHDC2   ZDHHC14    ZDHHC7      ZEB1     ZFP36      ZHX3    ZNF423  \n",
       "0  0.243935 -0.056318 -0.204971  0.179639 -0.292136 -0.034261  0.490152  \n",
       "1  0.551354  0.215982  0.196677  1.467320  2.461040  0.415041  2.116880  \n",
       "2  0.010498  0.592285 -0.338954 -0.842242  0.096242 -0.471005 -1.662190  \n",
       "3  0.760080 -1.169030  0.325604  1.785760 -0.212328  0.537493 -0.102138  \n",
       "4 -0.551470 -0.188697  0.157393  0.469166  1.748000  0.144196 -0.561641  \n",
       "\n",
       "[5 rows x 322 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read the dataset\n",
    "dt = pd.read_csv('hgsc.csv')\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt=dt.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now x, y\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "X = dt.loc[:,dt.columns!='class']\n",
    "#dt['class']=dt['class'].astype('integer') # the following is useless.. We can use LabelEncoder from sklearn.preprocessing\n",
    "y = dt['class']\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(489, 321)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now split in test and train set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us define 4 pipelines. Start with the SVM pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import svm\n",
    "\n",
    "pipe_svm = Pipeline([('scl', StandardScaler()),\n",
    "                   ('clf', svm.SVC(C=1.0, kernel='linear', probability=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define a LDA pipeline.\n",
    "#from sklearn.lda import LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "pipe_lda = Pipeline([('sc', StandardScaler()),\n",
    "                    ('clf', LDA())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next define the QDA pipeline\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "\n",
    "pipe_qda = Pipeline([('sc', StandardScaler()),\n",
    "                    ('clf', QDA())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last, let's define the Random Forest Pipeline\n",
    "\n",
    "from  sklearn.ensemble import RandomForestClassifier as RFC\n",
    "\n",
    "pipe_rf = Pipeline([('sc', StandardScaler()),\n",
    "                    ('clf', RFC())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexzucca/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/alexzucca/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('sc', StandardScaler(copy=True, with_mean=True, with_std=True)), ('clf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "         ...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's try to use the above pipelines\n",
    "#SVM\n",
    "pipe_svm.fit(X_train, y_train)\n",
    "#LDA\n",
    "pipe_lda.fit(X_train, y_train)\n",
    "#QDA\n",
    "pipe_qda.fit(X_train, y_train)\n",
    "#RF\n",
    "pipe_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the accuracies of each classifier\n",
    "svm_acc = pipe_svm.score(X_test, y_test)\n",
    "lda_acc = pipe_lda.score(X_test, y_test)\n",
    "qda_acc = pipe_qda.score(X_test, y_test)\n",
    "rf_acc  = pipe_rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy:  0.85306122449\n",
      "LDA accuracy:  0.563265306122\n",
      "QDA accuracy:  0.338775510204\n",
      "RFC accuracy:  0.804081632653\n"
     ]
    }
   ],
   "source": [
    "# Now print the accuracies\n",
    "print('SVM accuracy: ', svm_acc)\n",
    "print('LDA accuracy: ', lda_acc)\n",
    "print('QDA accuracy: ', qda_acc)\n",
    "print('RFC accuracy: ', rf_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First trial: there is no J-folding here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to code the stacked ensemble classifier\n",
    "\n",
    "# Before implementing the actual J-fold methid, I want to train and predict on the same thing... just to practice.\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.externals import six\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import _name_estimators\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "class StackedEnsembleClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, lev0_clfs, lev1_clf, J ,weights = None):\n",
    "        self.lev0_clfs = lev0_clfs\n",
    "        self.named_clfs = {key: value for key, value in _name_estimators(lev0_clfs)}\n",
    "        self.lev1_clf = lev1_clf\n",
    "        self.weights = weights\n",
    "        # number of folds\n",
    "        self.J = J\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "                \n",
    "        # lev0_clfs_ is the set of fitted classifiers.\n",
    "        self.lev0_clfs_ = []\n",
    "        \n",
    "        # Now fit each classifier\n",
    "        for clf in self.lev0_clfs:\n",
    "            fitted_clf = clf.fit(X,y)\n",
    "            self.lev0_clfs_.append(fitted_clf)\n",
    "        \n",
    "        # Initialize the array for the level 1 classifier\n",
    "        rows = X.shape[0]\n",
    "        columns = len(np.unique(y))*len(self.lev0_clfs)\n",
    "        \n",
    "        self.num_classes = len(np.unique(y))\n",
    "        \n",
    "        X2=np.zeros((rows, columns))\n",
    "        \n",
    "        # Prepare the data for the level 1 classifier\n",
    "        for i in range(len(self.lev0_clfs_)):\n",
    "            clf = self.lev0_clfs_[i]\n",
    "            X_temp=clf.predict_proba(X)\n",
    "            for j in range(X_temp.shape[1]):\n",
    "                for k in range(X2.shape[0]):\n",
    "                    c = len(np.unique(y))*i+j\n",
    "                    X2[k][c] = X_temp[k][j]\n",
    "        \n",
    "        \n",
    "        # now train the level 1 classifier\n",
    "        self.lev1_clf_ = self.lev1_clf.fit(X2,y)\n",
    "        \n",
    "        # That's it        \n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        \n",
    "        rows = X.shape[0]\n",
    "        columns = self.num_classes*len(self.lev0_clfs_)\n",
    "    \n",
    "        X2=np.zeros((rows, columns))\n",
    "        print('X2.shape:',X2.shape)\n",
    "        \n",
    "        for i in range(len(self.lev0_clfs_)):\n",
    "            clf = self.lev0_clfs_[i]\n",
    "            X_temp=clf.predict_proba(X)\n",
    "            for j in range(X_temp.shape[1]):\n",
    "                for k in range(X2.shape[0]):\n",
    "                    c = len(np.unique(y))*i+j\n",
    "                    #print(k,c)\n",
    "                    X2[k][c] = X_temp[k][j]\n",
    "                    \n",
    "                    \n",
    "        # now feed this stuff into the lev1_classifier\n",
    "        y_pred = self.lev1_clf_.predict(X2)\n",
    "        return(y_pred)\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        # will modify this later.\n",
    "        \n",
    "        rows = X.shape[0]\n",
    "        columns = self.num_classes*len(self.lev0_clfs_)\n",
    "    \n",
    "        X2=np.zeros((rows, columns))\n",
    "        print('X2.shape:',X2.shape)\n",
    "        \n",
    "        for i in range(len(self.lev0_clfs_)):\n",
    "            clf = self.lev0_clfs_[i]\n",
    "            X_temp=clf.predict_proba(X)\n",
    "            for j in range(X_temp.shape[1]):\n",
    "                for k in range(X2.shape[0]):\n",
    "                    c = len(np.unique(y))*i+j\n",
    "                    #print(k,c)\n",
    "                    X2[k][c] = X_temp[k][j]\n",
    "                    \n",
    "                    \n",
    "        # now feed this stuff into the lev1_classifier\n",
    "        \n",
    "        p_pred = self.lev1_clf_.predict_proba(X)\n",
    "        \n",
    "        return(p_pred)\n",
    "    \n",
    "        # return prediction probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr1 = LogisticRegression()\n",
    "lr2 = LogisticRegression()\n",
    "svc1 = svm.SVC(C=1.0, kernel = 'linear', probability=True, random_state= 42)\n",
    "svc2 = svm.SVC(C=1.0, kernel = 'rbf', probability=True, random_state= 42)\n",
    "lda = LDA()\n",
    "qda = QDA()\n",
    "rf = RFC()\n",
    "\n",
    "#sec=StackedEnsembleClassifier([svc, lda, rf], lr)\n",
    "sec=StackedEnsembleClassifier([svc1, svc2], lr1, J=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try to fit the thing\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train = sc.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackedEnsembleClassifier(J=2,\n",
       "             lev0_clfs=[SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=True, random_state=42, shrinking=True,\n",
       "  tol=0.001, verbose=False), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=42, shrinking=True,\n",
       "  tol=0.001, verbose=False)],\n",
       "             lev1_clf=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "             weights=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now train the ensemble.\n",
    "sec.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X2.shape: (245, 8)\n"
     ]
    }
   ],
   "source": [
    "y_pred = sec.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = 1.0 - (y_pred != y_test).sum()/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy Stacked Ensemble: 0.873469387755\n"
     ]
    }
   ],
   "source": [
    "print('accuracy Stacked Ensemble:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy SVM1: 0.85306122449\n",
      "accuracy SVM2: 0.865306122449\n"
     ]
    }
   ],
   "source": [
    "# accuracy of a single thing\n",
    "svc1.fit(X_train, y_train)\n",
    "y_pred = svc1.predict(X_test)\n",
    "accuracy = 1.0 - (y_pred!=y_test).sum()/len(y_test)\n",
    "print('accuracy SVM1:', accuracy)\n",
    "\n",
    "svc2.fit(X_train, y_train)\n",
    "y_pred = svc2.predict(X_test)\n",
    "accuracy = 1.0 - (y_pred!=y_test).sum()/len(y_test)\n",
    "print('accuracy SVM2:', accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing J-foldings right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Trying to code the stacked ensemble classifier\n",
    "\n",
    "# Before implementing the actual J-fold methid, I want to train and predict on the same thing... just to practice.\n",
    "\n",
    "# Now J-folding\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.externals import six\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import _name_estimators\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "class StackedEnsembleClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, lev0_clfs, lev1_clf, J ,weights = None):\n",
    "        self.lev0_clfs = lev0_clfs\n",
    "        self.named_clfs = {key: value for key, value in _name_estimators(lev0_clfs)}\n",
    "        self.lev1_clf = lev1_clf\n",
    "        self.weights = weights\n",
    "        # number of folds\n",
    "        self.J = J\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # Initialize the k-fold\n",
    "        \n",
    "        # Initialize the Kfold stuff\n",
    "        #kf = KFold(n_splits = self.J, random_state=42)\n",
    "        \n",
    "        # lev0_clfs_ is the set of fitted classifiers.\n",
    "        self.lev0_clfs_ = []\n",
    "        \n",
    "        # Now fit each classifier\n",
    "        for clf in self.lev0_clfs:\n",
    "            fitted_clf = clf.fit(X,y)\n",
    "            self.lev0_clfs_.append(fitted_clf)\n",
    "            \n",
    "        #print('classifiers fitted..')\n",
    "        \n",
    "        \n",
    "        # Initialize the array for the level 1 classifier\n",
    "        rows = X.shape[0]\n",
    "        columns = len(np.unique(y))*len(self.lev0_clfs)\n",
    "        \n",
    "        self.num_classes = len(np.unique(y))\n",
    "        \n",
    "        X2=np.zeros((rows, columns))\n",
    "        #print('X2.shape:',X2.shape)\n",
    "\n",
    "\n",
    "        #first_iter = True\n",
    "        #ind=0\n",
    "        #for (train, test) in kf.split(X, y):\n",
    "        #    #print('k:',k)\n",
    "        #    ind+=1\n",
    "        #    print('ind:', ind)\n",
    "\n",
    "            # Initialize a temporary array\n",
    "        #    rows_temp = len(y[test])\n",
    "        #    X_temp = np.zeros((rows_temp, columns))\n",
    "        #    \n",
    "            # level 0 for the k-th fold\n",
    "        #    for i in range(len(self.lev0_clfs)):\n",
    "        #        self.lev0_clfs[i].fit(X[train], y[train])\n",
    "        #        X_temp2 = self.lev0_clfs[i].predict_proba(X[test])\n",
    "                \n",
    "        #        for n in range(X_temp.shape[0]):\n",
    "        #            for m in range(X_temp2.shape[1]):\n",
    "        #                c = len(np.unique(y))*i+m\n",
    "        #                X_temp[n][c] = X_temp2[n][m]\n",
    "        #                \n",
    "        #    if (first_iter):\n",
    "        #        X2 = X_temp\n",
    "        #        first_iter = False\n",
    "        #    else:\n",
    "        #        X2 = np.concatenate((X2,X_temp))\n",
    "              \n",
    "            \n",
    "        #print('X2.shape:', X2.shape)\n",
    "                \n",
    "                        \n",
    "        \n",
    "        \n",
    "        for i in range(len(self.lev0_clfs_)):\n",
    "            clf = self.lev0_clfs_[i]\n",
    "            X_temp=clf.predict_proba(X)\n",
    "            for j in range(X_temp.shape[1]):\n",
    "                for k in range(X2.shape[0]):\n",
    "                    c = len(np.unique(y))*i+j\n",
    "        #            #print(k,c)\n",
    "                    X2[k][c] = X_temp[k][j]\n",
    "        \n",
    "        \n",
    "        # now train the level 1 classifier\n",
    "        self.lev1_clf_ = self.lev1_clf.fit(X2,y)\n",
    "        \n",
    "        # That's it        \n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        \n",
    "        rows = X.shape[0]\n",
    "        columns = self.num_classes*len(self.lev0_clfs_)\n",
    "    \n",
    "        X2=np.zeros((rows, columns))\n",
    "        print('X2.shape:',X2.shape)\n",
    "        \n",
    "        for i in range(len(self.lev0_clfs_)):\n",
    "            clf = self.lev0_clfs_[i]\n",
    "            X_temp=clf.predict_proba(X)\n",
    "            for j in range(X_temp.shape[1]):\n",
    "                for k in range(X2.shape[0]):\n",
    "                    c = len(np.unique(y))*i+j\n",
    "                    #print(k,c)\n",
    "                    X2[k][c] = X_temp[k][j]\n",
    "                    \n",
    "                    \n",
    "        # now feed this stuff into the lev1_classifier\n",
    "        y_pred = self.lev1_clf_.predict(X2)\n",
    "        return(y_pred)\n",
    "        # return prediction\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        # will modify this later.\n",
    "        \n",
    "        rows = X.shape[0]\n",
    "        columns = self.num_classes*len(self.lev0_clfs_)\n",
    "    \n",
    "        X2=np.zeros((rows, columns))\n",
    "        print('X2.shape:',X2.shape)\n",
    "        \n",
    "        for i in range(len(self.lev0_clfs_)):\n",
    "            clf = self.lev0_clfs_[i]\n",
    "            X_temp=clf.predict_proba(X)\n",
    "            for j in range(X_temp.shape[1]):\n",
    "                for k in range(X2.shape[0]):\n",
    "                    c = len(np.unique(y))*i+j\n",
    "                    #print(k,c)\n",
    "                    X2[k][c] = X_temp[k][j]\n",
    "                    \n",
    "                    \n",
    "        # now feed this stuff into the lev1_classifier\n",
    "        \n",
    "        p_pred = self.lev1_clf_.predict_proba(X)\n",
    "        \n",
    "        return(p_pred)\n",
    "    \n",
    "        # return prediction probabilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
